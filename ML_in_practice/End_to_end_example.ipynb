{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Copy of End_to_end_example.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_9IavYxfqKF"
      },
      "source": [
        "# 12. Machine learning techniques"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1qnFjBIftmC"
      },
      "source": [
        "!git clone https://github.com/s7s/machine_learning_1.git\n",
        "%cd  machine_learning_1/ML_in_practice"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3nP5yDNfqKI"
      },
      "source": [
        "import random as rd\n",
        "rd.seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAbJWz2MfqKH"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8GM6HeSfqKI"
      },
      "source": [
        "## 12.1 Loading and exploring the dataset\n",
        "\n",
        "First, we use pandas to load the dataset from a csv file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFuOhKGrfqKJ"
      },
      "source": [
        "# use pabdas to read './titanic.csv'\n",
        "raw_data = \n",
        "raw_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8cSfkeYfqKL"
      },
      "source": [
        "Next, we can explore the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9A_Cb6afqKL"
      },
      "source": [
        "# Use pandas to examine some info of the dataset\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use pandas to examine the description of the dataset\n"
      ],
      "metadata": {
        "id": "ipnuOTVtK5mI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use pandas to examine the histograms of the dataset columns\n"
      ],
      "metadata": {
        "id": "F5x-1ojDLTVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pwL3iZNfqKN"
      },
      "source": [
        "# Use pandas to examine \"survived\" column (labels)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eT6BD83VfqKO"
      },
      "source": [
        "# Use pandas to exanine more than one column at the same time [\"Name\", \"Age\"]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87L5P5_GfqKN"
      },
      "source": [
        "# Use pandas to check how many passengers survived\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visulaization \n",
        "a=raw_data.where(raw_data[\"Survived\"]==0)[[\"Age\",\"Survived\"]]\n",
        "b=raw_data.where(raw_data[\"Survived\"]==1)[[\"Age\",\"Survived\"]]\n",
        "v1,b1,_=plt.hist(a[\"Age\"],bins=[0,10, 20, 30, 40,50,60,70,80]);\n",
        "v2,b2,_=plt.hist(b[\"Age\"],bins=[0,10, 20, 30, 40,50,60,70,80]);\n",
        "values=np.stack((v1,v2))\n",
        "stacked_data = 100*values /values.sum(axis=0)\n",
        "\n",
        "N=10\n",
        "ind = np.arange(N) # the x locations for the groups\n",
        "width = 7\n",
        "fig = plt.figure()\n",
        "ax = fig.add_axes([0,0,1,1])\n",
        "print(stacked_data[0,:])\n",
        "ax.bar(b1[:-1], stacked_data[0,:], width, color='r')\n",
        "ax.bar(b1[:-1], stacked_data[1,:], width,bottom=stacked_data[0,:], color='b')\n",
        "ax.set_title('All Data')\n",
        "ax.set_xlabel('Age')\n",
        "ax.set_xticks(b1[:-1])\n",
        "ax.set_yticks(np.arange(0, 81, 10))\n",
        "ax.legend(labels=['Not_Survived', 'Survived'])\n",
        "plt.show()\n",
        "\n",
        "\n",
        "a=raw_data.loc[(raw_data[\"Survived\"]==0) & (raw_data[\"Sex\"]==\"female\"),[\"Age\",\"Survived\"]]\n",
        "b=raw_data.loc[(raw_data[\"Survived\"]==1) & (raw_data[\"Sex\"]==\"female\"),[\"Age\",\"Survived\"]]\n",
        "v1,b1,_=plt.hist(a[\"Age\"],bins=[0,10, 20, 30, 40,50,60,70,80]);\n",
        "v2,b2,_=plt.hist(b[\"Age\"],bins=[0,10, 20, 30, 40,50,60,70,80]);\n",
        "values=np.stack((v1,v2))\n",
        "stacked_data = 100*values /values.sum(axis=0)\n",
        "\n",
        "N=10\n",
        "ind = np.arange(N) # the x locations for the groups\n",
        "width = 7\n",
        "fig = plt.figure()\n",
        "ax = fig.add_axes([0,0,1,1])\n",
        "print(stacked_data[0,:])\n",
        "ax.bar(b1[:-1], stacked_data[0,:], width, color='r')\n",
        "ax.bar(b1[:-1], stacked_data[1,:], width,bottom=stacked_data[0,:], color='b')\n",
        "ax.set_title('Females Only')\n",
        "ax.set_xlabel('Age')\n",
        "ax.set_xticks(b1[:-1])\n",
        "ax.set_yticks(np.arange(0, 81, 10))\n",
        "ax.legend(labels=['Not_Survived', 'Survived'])\n",
        "plt.show()\n",
        "\n",
        "a=raw_data.loc[(raw_data[\"Survived\"]==0) & (raw_data[\"Sex\"]==\"male\"),[\"Age\",\"Survived\"]]\n",
        "b=raw_data.loc[(raw_data[\"Survived\"]==1) & (raw_data[\"Sex\"]==\"male\"),[\"Age\",\"Survived\"]]\n",
        "v1,b1,_=plt.hist(a[\"Age\"],bins=[0,10, 20, 30, 40,50,60,70,80]);\n",
        "v2,b2,_=plt.hist(b[\"Age\"],bins=[0,10, 20, 30, 40,50,60,70,80]);\n",
        "values=np.stack((v1,v2))\n",
        "stacked_data = 100*values /values.sum(axis=0)\n",
        "\n",
        "N=10\n",
        "ind = np.arange(N) # the x locations for the groups\n",
        "width = 7\n",
        "fig = plt.figure()\n",
        "ax = fig.add_axes([0,0,1,1])\n",
        "print(stacked_data[0,:])\n",
        "ax.bar(b1[:-1], stacked_data[0,:], width, color='r')\n",
        "ax.bar(b1[:-1], stacked_data[1,:], width,bottom=stacked_data[0,:], color='b')\n",
        "ax.set_title('Males only')\n",
        "ax.set_xlabel('Age')\n",
        "ax.set_xticks(b1[:-1])\n",
        "ax.set_yticks(np.arange(0, 81, 10))\n",
        "ax.legend(labels=['Not_Survived', 'Survived'])\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "4lVlKAHLLuiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKYLTXINfqKO"
      },
      "source": [
        "## 12.2. Cleaning up the data\n",
        "\n",
        "Now, let's look at how many columns have missing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9azVLEVtfqKO"
      },
      "source": [
        "# use pandas to check missing data (NA(not available) values) for all the columns\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXFTGpN5fqKP"
      },
      "source": [
        "The Cabin column is missing too many values to be useful. Let's drop it altogether."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A49SL-0NfqKQ"
      },
      "source": [
        "# Use pandas to drop \"Cabin\" column\n",
        "clean_data = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2227vwAofqKQ"
      },
      "source": [
        "clean_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDdfjznYfqKQ"
      },
      "source": [
        "Other columns such as Age or Embarked are missing some values, but they can still be useful.\n",
        "\n",
        "For the age column, let's fill in the missing values with the median of all ages.\n",
        "\n",
        "For the Embarked column, let's make a new category called 'U', for Unknown port of embarkment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VesqtzGsfqKR"
      },
      "source": [
        "# get the median of age column using pandas\n",
        "median_age = \n",
        "median_age"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVksNV43fqKR"
      },
      "source": [
        "# use pandas to fill the na values in age column with the median age\n",
        "clean_data[\"Age\"] = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf_Z2CzvfqKR"
      },
      "source": [
        "# use pandas to fill the na values in embarked column with 'U'\n",
        "\n",
        "clean_data[\"Embarked\"] = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAL-CsGyfqKR"
      },
      "source": [
        "clean_data.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ks6LMctufqKS"
      },
      "source": [
        "# view 10 rows of the clean dataset\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUfU0glCfqKS"
      },
      "source": [
        "### 12.2.3 Saving our data for the future"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtAY8WYTfqKS"
      },
      "source": [
        "# save the clean dataset to './clean_titanic_data.csv'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Y_4pk_RfqKS"
      },
      "source": [
        "## 12.3 Manipulating the features\n",
        "\n",
        "- One-hot encoding\n",
        "- Binning\n",
        "- Feature selection\n",
        "\n",
        "### 12.3.1 One-hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8pojgwefqKS"
      },
      "source": [
        "preprocessed_data = pd.read_csv('clean_titanic_data.csv')\n",
        "preprocessed_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZivvKrlcfqKS"
      },
      "source": [
        "# Use pandas method .get_dummies() to get the one hot encoding of “embarked”, “pclass” and “gender”\n",
        "\n",
        "\n",
        "\n",
        "# Use pandas method .drop() to remove the old columns and method .concat() to add the new columns\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwEwCHPFfqKT"
      },
      "source": [
        "preprocessed_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cV-AbnysfqKU"
      },
      "source": [
        "### 12.3.2 Binning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gThz2CLlfqKU"
      },
      "source": [
        "bins = [0, 10, 20, 30, 40, 50, 60, 70, 80]\n",
        "# Use .cut() method to make bins from the age column\n",
        "\n",
        "categorized_age = \n",
        "preprocessed_data['Categorized_age'] = categorized_age\n",
        "preprocessed_data = preprocessed_data.drop([\"Age\"], axis=1)\n",
        "preprocessed_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBtwwk_ofqKU"
      },
      "source": [
        "# Use pandas method .get_dummies() to get the one hot encoding of “Categorized_age”\n",
        "# Use pandas method .drop() to remove the old column and method .concat() to add the new columns\n",
        "\n",
        "\n",
        "\n",
        "preprocessed_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8251vH3fqKV"
      },
      "source": [
        "### 12.3.4 Feature selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_SErhJ9fqKV"
      },
      "source": [
        "# drop these columns['Name', 'Ticket', 'PassengerId']\n",
        "\n",
        "\n",
        "\n",
        "preprocessed_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw-TKDt1fqKV"
      },
      "source": [
        "### 12.3.5 Saving for future use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzlAdRmffqKW"
      },
      "source": [
        "preprocessed_data.to_csv('./preprocessed_titanic_data.csv', index=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bS3XGQrXfqKW"
      },
      "source": [
        "# 12.4 Training models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnmsC1G7fqKW"
      },
      "source": [
        "data = pd.read_csv('./preprocessed_titanic_data.csv')\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akXchKUDfqKW"
      },
      "source": [
        "### 12.4.1 Features-labels split and train-validation split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lzo2QNufqKX"
      },
      "source": [
        "# drop [\"Survived\"] column and save that to features\n",
        "features = \n",
        "\n",
        "# save the [\"Survived\"] column to labels\n",
        "labels = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-gIWUBhfqKY"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3CZd8UGfqKY"
      },
      "source": [
        "# split data by 60% train ; use random_state=100\n",
        "features_train, features_validation_test, labels_train, labels_validation_test = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZXTP5lgfqKY"
      },
      "source": [
        "# split test data by 50% validation and 50% test ; use random_state=100\n",
        "features_validation, features_test, labels_validation, labels_test = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCRpVXr0fqKY"
      },
      "source": [
        "print(len(features_train))\n",
        "print(len(features_validation))\n",
        "print(len(features_test))\n",
        "print(len(labels_train))\n",
        "print(len(labels_validation))\n",
        "print(len(labels_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dJMAfTUfqKZ"
      },
      "source": [
        "### 12.4.2 Training different models on our dataset\n",
        "\n",
        "We'll train six models:\n",
        "- Logistic regression (perceptron)\n",
        "- Decision tree\n",
        "- Support vector machine (SVM)\n",
        "- RandomForestClassifier\n",
        "- GradientBoostingClassifier\n",
        "- AdaBoostClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gP26kwwfqKZ"
      },
      "source": [
        "# Train logistic regression model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr_model = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DThp9iwIfqKZ"
      },
      "source": [
        "# Train decision tree model ; don't use any hyperparameter\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "dt_model = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yMO82c9fqKa"
      },
      "source": [
        "# Train SVM model ; don't use any hyperparameter\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "svm_model = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhIfxavqfqKa"
      },
      "source": [
        "# Train random forest model ; don't use any hyperparameter\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf_model = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EW7wDPHIfqKa"
      },
      "source": [
        "# Train gradient boosting model ; don't use any hyperparameter\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "gb_model = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPmyHC-ZfqKa"
      },
      "source": [
        "# Train Adaboost model ; don't use any hyperparameter\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "ab_model = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jF0KbS_fqKb"
      },
      "source": [
        "### 12.4.3 Evaluating the models\n",
        "\n",
        "#### Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjeMJk4KfqKb"
      },
      "source": [
        "# print accuracy of each model on validation data\n",
        "print(\"Scores of the models\")\n",
        "print(\"Logistic regression:\", )\n",
        "print(\"Decision tree:\", )\n",
        "print(\"SVM:\", )\n",
        "print(\"Random forest:\", )\n",
        "print(\"Gradient boosting:\", )\n",
        "print(\"AdaBoost:\", )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCUK9NJXfqKb"
      },
      "source": [
        "#### F1-score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QU9uZMJkfqKb"
      },
      "source": [
        "# print F1-score of each model on validation data\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "print(\"F1-scores of the models:\")\n",
        "\n",
        "print(\"Logistic regression:\", )\n",
        "\n",
        "print(\"Decision tree:\", )\n",
        "\n",
        "print(\"SVM:\", )\n",
        "\n",
        "print(\"Random forest:\", )\n",
        "\n",
        "print(\"Gradient boosting:\", )\n",
        "\n",
        "print(\"AdaBoost:\", )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN_q429mfqKb"
      },
      "source": [
        "### 12.4.4 Testing the model\n",
        "\n",
        "Finding the accuracy and the F1-score of the model in the testing set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nfxWhppfqKb"
      },
      "source": [
        "# print accuracy of gradient boost model on testing data\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oW0DEPGxfqKc"
      },
      "source": [
        "# print F1-score of gradient boost model on testing data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1XGztd2fqKc"
      },
      "source": [
        "# 12.5 Grid search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAK1vV5WfqKc"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrcPjLM1fqKc"
      },
      "source": [
        "svm_parameters = {'kernel': ['rbf'],\n",
        "                  'C': [0.01, 0.1, 1 , 10, 100],\n",
        "                  'gamma': [0.01, 0.1, 1, 10, 100]\n",
        "                }\n",
        "# use gridsearch to find the best hyperparameters \n",
        "svm = SVC()\n",
        "svm_gs = \n",
        "\n",
        "# git the best model\n",
        "svm_winner = \n",
        "svm_winner\n",
        "\n",
        "svm_winner.score(features_validation, labels_validation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmtr0bLVfqKd"
      },
      "source": [
        "svm_winner"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDqUNXY1fqKd"
      },
      "source": [
        "# 12.6 Cross validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqi1xhi0fqKd"
      },
      "source": [
        "# print the k-fold cross validation output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12.7 Save the model"
      ],
      "metadata": {
        "id": "4DhPEZv_SCjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use joblib to save the model\n"
      ],
      "metadata": {
        "id": "SVgPG_UrSLXC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}